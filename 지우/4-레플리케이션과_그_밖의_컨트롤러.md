# 4장: 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 4.1 파드의 안정적인 유지

📍컨테이너에 크래시가 발생하거나 라이브니스 프로브(liveness probe)가 실패한 경우

⇒ kubelet이 컨테이너를 다시 시작 (컨트롤플레인은 관여 X)

📍노드 자체에 크래시가 발생한 경우

⇒ 모든 파드의 대체 파드를 생성하는 것은 컨트롤플레인의 몫

📍애플리케이션이 다른 노드에서 실행되도록 하려면?

⇒ 레플리케이션컨트롤러와 같은 메커니즘으로 파드 관리

### 라이브니스 프로브 (Liveness Probe)

> _k8s의 health check를 통해 정상적이지 못한 경우 컨테이너를 재시작할 수 있게 하는 메커니즘_

- 세 가지 메커니즘
  - HTTP GET 프로브: 지정 IP 주소, 포트, 경로에 HTTP GET 요청을 수행한다.
    - 서버 오류 응답 코드 반환 혹은 무응답 시, 컨테이너 재시작
  - TCP 소켓 프로브: 컨테이너의 지정포트에 TCP 연결을 시도하여 실패 시 컨테이너 재시작
  - Exec 프로브: 컨테이너 내 임의의 명령을 실행하고 명령의 종료 상태 코드가 0이 아니면 실패로 간주

```yaml
# liveness-probe.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
    - image: { image_name }
      name: kubia
      livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 15
```

- `initialDelaySeconds`: 초기 지연 설정값을 지정하지 않으면 컨테이너 시작과 동시에 프로브 시작
  - 대부분 애플리케이션이 바로 요청을 받을 준비가 되어있지 않으므로 프로브 실패 ⇒ 반드시 필요!

```yaml
# pod 배포
kubectl apply -f liveness-probe.yaml

# 파드 확인
kubectl get po kubia-liveness

# background log 확인
kubectl describe po kubia-liveness
```

배포 후 파드의 상세 정보 중 RESTARTS 값 확인

<aside>
💬 **Liveness Probe 정리**

1. prod 환경에서 실행 중인 파드는 반드시 liveness probe를 정의!
2. 서버의 응답 여부 확인 + 세 메커니즘에 따라 컨테이너 재시작
3. 외부 요인의 영향을 받지 않도록 하기
4. 가볍게 유지하기 → 일이 많으면 컨테이너 속도도 저하
5. 재시도 루프는 구현하지 말기 → 실폐 임계값이 1이어도 k8s는 프로브를 여러 번 재시도한다

</aside>

---

## 4.2 레플리케이션컨트롤러

> _파드가 항상 실행되도록 보장하며 지속적인 모니터링으로 실제 파드 수가 의도하는 수와 일치하는지 확인_

Node 1이 다운되면?

- Pod A는 직접 생성한 파드 → 완전 유실
- Pod B는 RC에 의해 Node 2에 새로 생성됨
  - RC가 실행 중인 모든 파드의 교체 복제본 생성
  - 파드가 삭제되면 RC에게 통보 → RC는 파드 수 확인하고 조치

<img src="https://user-images.githubusercontent.com/70079416/219946520-f4c9756a-323a-4ce1-b013-e5ca1b3ddd6e.pngg" width="70%" height="70%" />

**레플리케이션컨트롤러의 세 가지 요소**

- 레이블 셀렉터(파드 셀렉터): 레플리케이션컨트롤러의 범위에 있는 파드 결정
- 레플리카 수: 실행할 파드의 desired 수
  - 유일하게 기존 파드에 영향을 주는 요소
- 파드 템플릿: 새로 파드 레플리카 생성할 때 사용

<img src="https://user-images.githubusercontent.com/70079416/219946515-5fdeb818-b8ca-4118-beb6-fd669891dfaa.png" width="70%" height="70%" />
레이블 셀렉터 변경 시 → 기존 파드가 RC 범위를 벗어나 컨트롤러가 해당 파드에 대한 관리를 중지

### RC 생성

> _RC 정의파일에 파드 셀렉터를 지정하지 않는다 → k8s가 파드 템플릿에서 추출하도록_

파드 템플릿의 파드 레이블과 RC의 레이블 셀렉터가 완전히 일치해야 한다 ⇒ 그렇지 않으면 새 파드를 무한생성

### RC 동작원리

> _파드 삭제 자체가 아닌 파드가 부족한 결과적인 상태에 대응하여 새 파드를 생성_

기존 레이블에 새로운 레이블을 추가하면? ⇒ RC의 범위에 영향을 미치지 않으므로 아무 변화 없음

기존 레이블을 변경하면? ⇒ 레이블이 변경된 파드는 RC의 범위를 벗어나 RC가 관리하지 않는 파드가 되고, RC는 레플리카 수에 맞게 기존 레이블의 새 파드를 생성한다.

📍파드 레이블 대신 RC의 레이블 셀렉터를 수정하면?

모든 파드가 RC의 범위를 벗어나 레플리카 수에 맞게 새 파드를 생성한다.

📍파드 템플릿을 변경하면? (177p.)

⇒ 기존 파드에는 아무 영향 없다

---

## 4.3 레플리카셋

> _차세대 레플리케이션컨트롤러 (완전히 대체할 것)_

| 레플리케이션컨트롤러                               | 레플리카셋                                               |
| -------------------------------------------------- | -------------------------------------------------------- |
| 특정 레이블이 있는 파드만 매칭 가능                | 특정 레이블이 없는 파드와 매칭 가능                      |
| 레이블이 evn=prod, env=dev인 파드 동시 매칭 불가능 | 하나의 레플리카셋으로 두 파드 세트 모두 매칭 가능        |
| 레이블 키 존재만으로 파드 매칭 불가능              | 레이블값 상관없이 특정 레이블 키를 갖는 파드와 매칭 가능 |

```yaml
# kubia-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  elector:
    app: kubia
template:
  metadata:
    labels:
      app: kubia
  spec:
    containers:
      - name: kubia
        image: gwmelody/kubia
        ports:
          - containerPort: 8080
```

```yaml
# kubia-rc.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
template:
  metadata:
    labels:
      app: kubia
  spec:
    containers:
      - name: kubia
        image: gwmelody/kubia
```

### 표현적인 레이블 셀렉터

```yaml
---
selector:
  matchExpressions:
    - key: app
      operator: In
      values:
        - kubia
```

⇒ 파드의 키가 app인 레이블 포함하고, 레이블의 값은 kubia여야 한다

- `In` / `NotIn`: 레이블의 값이 지정된 값 중 하나와 일치/불일치
- `Exists` / `DoesNotExist`: 지정된 키를 가진 레이블이 포함/미포함

---

## 4.4 데몬셋

| 레플리카셋                                   | 데몬셋                                                               |
| -------------------------------------------- | -------------------------------------------------------------------- |
| 클러스터 내 어딘가에 지정된 수만큼 파드 실행 | 클러스터의 모든 노드에, 노드당 하나의 파드만 실행                    |
| 원하는 수의 파드 복제본이 존재하는지 확인    | 원하는 복제본 수 개념 X                                              |
| 레플리카 수에 맞게 파드 생성                 | 다른 곳에 파드 생성 X                                                |
|                                              | 새 노드가 클러스터에 추가되면 즉시 새 파드 인스턴스를 새 노드에 배포 |
|                                              | 얘도 파드 템플릿 존재                                                |

<img src="https://user-images.githubusercontent.com/70079416/219946518-56adf232-d8ad-4580-bf7b-c791fc4f6c10.png" width="70%" height="70%" />

---

## 4.5 완료 가능한 태스크에서의 파드

### Job 리소스

파드의 컨테이너 내부에서 실행 중인 프로세스가 완료되면 컨테이너를 재시작하지 않는 파드 실행

- 노드에 장애 발생 시 → 잡이 관리하고 있던 파드는 다른 노드로 스케줄링
- 프로세스 자체에 장애가 생길 시 → 잡에서 컨테이너 재시작 여부를 설정
- [spec] > [restartPolicy] : `OnFailure`/`Never`로 명시적 설정 (`Always`로 설정X)

### 잡에서 여러 파드 인스턴스 실행하기

> _Job spec에 `completions`와 `parallelism` 속성 설정_

- `completions` → 순차적으로 실행할 파드 수 정의
- `parallelism` → 병렬로 실행할 수 있는 파드 수 정의

**잡 스케일링**

```bash
# 맨 뒤에 parallelism 속성 변경 가능 → 그 수만큼 파드 실행
kubectl scale {job_name} --replicas 3
```

**잡 파드의 완료 시간 제한하기**

[spec]에 `activeDeadlineSeconds` 속성으로 시간 제한

- 더 오래 실행되면 시스템 종료 및 잡 실패로 표시

---

## 4.6 잡을 주기적으로 실행하기

### CronJob 리소스

> _특정 시간 또는 지정된 간격으로 파드를 반복 실행_

```yaml
# cronjob.yaml
apiVersion: batch/v1
kind: CronJob
---
spec:
  schedule: " "
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: { name }
        spec:
          restartPolicy: OnFailure
          containers:
```
