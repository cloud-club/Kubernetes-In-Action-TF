# 5장: 서비스: 클라이언트가 파드를 검색하고 통신을 가능하게 함

## Background

대부분 애플리케이션은 외부와 통신하는데, 마이크로서비스의 경우 파드는 대개 클러스터 내부의 다른 파드나 클러스터 외부의 클라이언트에서 오는 HTTP 요청에 응답한다.

#### 파드에 직접 접근할 수 없는 3가지 이유

- 파드는 일시적
- 파드의 IP는 파드 시작 직전에 할당되므로 클라이언트가 미리 알 수 없음
- 여러 파드가 단일 IP 주소로 접근할 수 있어야 함

⇒ 이 문제를 해결하기 위해 서비스 리소스를 활용한다.

## 5.1 서비스

> _동일 서비스를 제공하는 파드 그룹에 지속적인 단일 접점을 만들기 위해 사용하는 리소스
> ⇒ 파드에 안정적인 IP 주소와 포트를 할당_

- 서비스는 하나 이상의 파드를 지원 가능
- 서비스 연결은 서비스 뒷단의 모든 파드로 로드밸런싱

**레이블 셀렉터**로 특정 파드가 어떤 서비스의 일부인지 정의할 수 있다!

### 서비스 생성

방법① `kubectl expose`를 사용하여 서비스 생성하고 모든 파드를 단일 IP 주소와 포트로 노출

방법② k8s API 서버에 yaml 파일로 서비스 정의

```yaml
# kubia-svc.yaml
apiVersion: v1
kind: Service
metadata:
name: kubia
spec:
  ports:
    - port: 80 # 서비스가 사용할 포트
      targetPort: 8080 # 서비스가 포워드할 포트
      selector:
        app: kubia # app=kubia 레이블의 모든 파드가 이 서비스에 포함
```

```bash
kubectl create -f kubia-svc.yaml
```

### 클러스터 내에서 실행 중인 컨테이너에 원격으로 서비스 접근

`kubectl exec` → 컨테이너의 내용, 상태, 환경 검사 시에 유용

```bash
# 파드와 서비스의 Cluster IP 확인
kubectl get po
kubectl get svc

# 더블대쉬(--) 뒤의 명령은 파드 내에서 실행되어야 하는 명령어
kubectl exec {pod_name} --curl -s http://{clusterIP}
```

<img src="https://user-images.githubusercontent.com/70079416/220809982-d1f21857-0620-410e-8845-6bd098c9c573.png" width="50%" height="50%" />

curl을 활용한 서비스 연결 테스트 동작원리

<aside>
💫 curl은 HTTP 요청을 서비스 IP로 전달
k8s의 서비스 프록시가 연결을 가로채서 임의의 파드로 요청을 전달
해당 파드 내에서 실행 중인 NodeJS가 요청 처리 후 HTTP 응답 반환
curl은 표준출력로 응답 출력

</aside>

### 서비스의 sessionAffinity

이건 임의의 파드에 연결해서 curl로 요청 확인하는 과정 → 요청할 때마다 다른 파드가 선택된다

⇒ 모든 요청을 매번 같은 파드로 리디렉션하려면 `sessionAffinity` 속성을 `ClientIP`로 설정한다!

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
```

- 동일 서비스를 여러 포트로 노출

  - port name을 지정해야 한다.

  ```yaml
  ---
  spec:
  ports:
    - name: http
      port: 80 # 서비스가 사용할 포트
      targetPort: 8080 # 서비스가 포워드할 포트
    - name: https
      port: 443 # 서비스가 사용할 포트
      targetPort: 8443
  selector:
    app: kubia
  ```

- 포트에 이름을 지정
  ```yaml
  kind: Pod
  spec:
    containers:
      - name: kubia
        ports:
          - name: http
            containerPort: 8080
          - name: https
            containerPort: 8443
  ```
  ```yaml
  apiVersion: v1
  kind: Service
  spec:
    ports:
      - name: http
        port: 80
        targetPort: http
      - name: https
        port: 443
        targetPort: https
  ```
  ⇒ 나중에 서비스가 아닌 파드 스펙을 변경함으로써 포트 번호를 바꿀 수 있음

### 서비스 검색

방법① 환경변수로 서비스 IP 주소와 포트 정보 등록

방법② DNS 서버를 활용

- `kube-system` ns의 `kube-dns` 파드는 dns 서버를 실행

방법③ FQDN을 통한 서비스 연결

방법④ 파드 컨테이너 내에서 `kubectl exec` 명령어로 bash와 같은 shell 실행

---

## 5.2 클러스터 외부에 있는 서비스 연결

### ① 파드 셀렉터로 서비스 엔드포인트 리소스 생성

```bash
# service endpoint 확인
kubectl describe svc {svc_name}
```

- `Selector`: 파드 셀렉터는 엔드포인트 목록을 생성하는 데 사용
  - 서비스 정의 시, 파드 셀렉터를 정의하지 않으면 엔드포인트 리소스가 자동생성되지 않는다
- `Endpoints`: 해당 서비스의 엔드포인트를 나타내는 파드 IP와 포트 목록
  ```bash
  # endpoint 정보만 출력
  kubectl get endpoints {svc_name}
  ```

### ② 수동으로 엔드포인트 구성

```yaml
apiVersion: v1
kind: Endpoints
metadata:
name: { svc_name }
subsets:
  - addresses:
      - ip: 11.11.11.11
      - ip: 22.22.22.22
    ports:
      - port: 80
```

- 엔드포인트의 오브젝트명은 서비스명과 일치해야 한다.
- 서비스와 엔드포인트 모두 서버에 게시하면 일반 서비스처럼 사용 가능 → 수동 배포 완료!
- 엔드포인트 yaml에 정의되어 있는 IP주소와 포트 쌍에 대한 모든 연결은 서비스 엔드포인트 간에 로드밸런싱

<img src="https://user-images.githubusercontent.com/70079416/220809963-7c281cbe-54e8-4927-b551-b1c2d7ebc03e.png" width="50%" height="50%" />

### ③ ExternalName 서비스 생성

서비스의 엔드포인트를 수동으로 구성하는 두번째 방법보다 FQDN(Fully Qualified Domain Name)으로 외부 서비스 참조하는 방식이 더 간단

- DNS 레벨에서만 구현되고, 클라이언트는 서비스 프록시를 완전히 무시하고 외부 서비스에 직접 연결되므로 ExternalName 유형의 서비스는 Cluster IP를 얻지 못한다.

---

## 5.3 외부 클라이언트에 서비스 노출

<aside>
💫 방법① NodePort
방법② LoadBalaner
방법③ Ingress

</aside>

### NodePort 서비스

모든 노드에 특정 포트를 할당

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30123
  selector:
    app: kubia
```

- minikube 사용 시
  ```bash
  minikube service {svc_name} -n {ns_name}
  ```
- GKE 사용 시 외부에서 노드포트 서비스에 접근할 수 있도록 방화벽 규칙 변경
  ```bash
  gcloud compute firewall-rules create {svc-rule_name} --allow=tcp:{nodeport_port}
  ```

### LoadBalancer 서비스

클라이언트가 요청을 보내는 노드에 장애가 나면 더 이상 서비스에 접근할 수 없다

⇒ 모든 노드에 요청을 분산시키고 오프라인 상태의 노드에는 요청을 보내지 않도록 하는 로드밸런서 배치

<aside>
‼️ minikube는 LoadBalancer 타입을 지원하지 않는다

</aside>

```yaml
# svc-loadbalancer.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
    - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

```bash
kubectl create -f svc-loadbalancer.yaml
```

⇒ 배포 완료 시 external IP가 할당되고 curl 통신으로 서비스에 액세스할 수 있다. (방화벽 설정도 따로 필요 X)

<img src="https://user-images.githubusercontent.com/70079416/220809968-9c258b42-7fa6-4a58-9ca1-4f7337cc5a1b.png" width="50%" height="50%" />

nodeport

<img src="https://user-images.githubusercontent.com/70079416/220809970-ee587373-2127-4fdf-9ad2-7a16d6b3605b.png" width="50%" height="50%" />

load balancer

---

## 5.4 인그레스 리소스

[인그레스(Ingress)](https://kubernetes.io/ko/docs/concepts/services-networking/ingress/)는 클러스터 외부에서 클러스터 내부 서비스로 HTTP와 HTTPS 경로를 노출한다. 트래픽 라우팅은 인그레스 리소스에 정의된 규칙에 의해 컨트롤된다.

<img src="https://user-images.githubusercontent.com/70079416/220809974-0269362c-7724-49f8-8986-06757d7cce8f.png" width="50%" height="50%" />

> 인그레스는 한 IP 주소로 여러 서비스에 접근이 가능하다.
> 클라이언트가 인그레스에 HTTP 요청을 보낼 때, 요청한 호스트와 경로에 따라 요청을 전달할 서비스 결정됨

<img src="https://user-images.githubusercontent.com/70079416/220809976-ce31afe2-7daf-4a67-800e-3af5034b8589.png" width="50%" height="50%" />

파드 셀렉터와 서비스의 레이블 셀렉터가 동일하면 해당 서비스의 엔드포인트로 파드가 포함된다.

<aside>
💬 minikube는 기본 인그레스 컨트롤러 제공 X, 인그레스 기능을 시험해볼 수 있는 애드온 제공 O

</aside>

```bash
# 애드온 list 확인하여 ingress: disabled 체크하고 ingress 활성화
minikube addons list
minikube addons enable ingress
```

### 인그레스 생성

```yaml
# kubia-ingress.yaml
apiVersion: extensions/v1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
    - host: kubia.example.com
      http:
      paths:
        - path: /
          backend:
            serviceName: kubia-nodeport
            servicePort: 80
```

`kubia.example.com` host로 요청되는 인그레스 컨트롤러에 수신된 모든 HTTP 요청을 80번 포트의 kubia-nodeport 서비스로 전송하는 인그레스 규칙 정의

### 인그레스로 서비스 액세스

1. 도메인 이름이 인그레스 컨트롤러의 IP와 매핑되도록 확인

```bash
kubectl get ingresses
# ADDRESS에서 IP 확인 가능
```

1. 도메인이 해당 IP로 확인하도록 DNS 서버 구성하거나 /etc/hosts에 IP와 도메인 추가

```bash
{ip_addr}    {domain_name}
# 192.xx.xx.xx   kubia.example.com
```

curl 명령어로 액세스

```yaml
curl http://{domain_name}
```

### 인그레스 동작

인그레스 컨트롤러는 요청을 서비스로 전달하지 않고 파드를 선택하는 데에만 사용한다.

### 하나의 인그레스로 여러 서비스 노출

⍢ 동일 호스트의 다른 경로로 여러 서비스 매핑

`.spec.rules[].http.paths`에 `path`, `backend`값 추가

- example
  ```yaml

  ---
  spec:
    rules:
      - host: kubia.example.com
        http:
        paths:
          - path: /kubia
            backend:
              serviceName: kubia
              servicePort: 80
          - path: /foo
            backend:
              serviceName: foo
              servicePort: 80
  ```

⍢ 서로 다른 호스트로 서로 다른 서비스 매핑

`.spec.rules[].host` 추가

- example
  ```yaml
  ...
  spec:
    rules:
    - host: foo.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: foo
            servicePort: 80
        - host:
          http:
            paths: bar.example.com
            - path: /
              backend:
                serviceName: bar
                servicePort: 80
  ```
  - 호스트 header에 따라 foo나 bar로 요청 전달
  - DNS는 두 도메인 이름을 모두 인그레스 컨트롤러의 IP 주소로 지정해야 한다.

### TLS 트래픽 처리

클라이언트 - 컨트롤러 간 통신 암호화 OK, 컨트롤러 - 백엔드 파드 간 통신은 암호화 X

⇒ 컨트롤러가 TLS 트래픽을 처리하도록 개인키와 인증서를 시크릿으로 정의하여 인그레스에 첨부

```bash
openssl genrsa -out tls.key 2048
openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj

kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
# kubectl create secret tls {secret_name} --cert={cert_name} --key={key_name}
```

```yaml
# kubia-ingress-tls.yaml
apiVersion: extensions/v1
kind: Ingress
metadata:
  name: kubia
spec:
  tls:
    - hosts:
        - kubia.example.com
      secretName: tls-secret
  rules:
    - host: kubia.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: kubia-nodeport
              servicePort: 80
```

---

## 5.5 레디니스 프로브

> _주기적으로 호출되며 특정 파드가 클라이언트 요청을 수신할 수 있는지 결정_

- 세 가지 메커니즘
  - Exec 프로브: 컨테이너 상태를 프로세스의 종료 상태 코드로 결정
  - HTTP GET 프로브: GET 요청을 컨테이너로 보내고 응답의 HTTP 상태 코드에 따라 컨테이너의 준비 여부 결정
  - TCP 소켓 프로브: 컨테이너의 지정된 포트로 TCP 연결을 열고 연결되면 컨테이너 준비 완료로 간주

### 레디니스 프로브 동작

| 라이브니스 프로브                                                                          | 레디니스 프로브                                                            |
| ------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------- |
| k8s의 health check를 통해 정상적이지 못하면 컨테이너를 재시작                              | 컨테이너가 준비 상태 점검에 실패해도 컨테이너가 종료되거나 재시작하지 않음 |
| health check에 실패한 컨테이너를 제거하고 새로운 컨테이너로 교체 ⇒ 파드 상태 정상으로 유지 | 요청을 처리할 준비가 된 파드의 컨테이너만 요청을 수신하도록 함             |

<img src="https://user-images.githubusercontent.com/70079416/220809979-3c5f6c26-b5c0-4573-883b-01a96cce1514.png" width="50%" height="50%" />

- 레디니스 프로브 실패 시, 파드는 엔드포인트 오브젝트에서 제거
- 서비스로 연결하는 클라이언트의 요청은 파드로 전달되지 않는다.

#### 레디니스 프로브가 중요한 이유?

클라이언트가 정상적인 파드와 통신하고 시스템에 문제가 있다는 것을 절대 알아차리지 못한다!

### 레디니스 프로브 정의

- `.spec.template.spec.containers`의 첫 번째 컨테이너에 추가
- 파드는 레디니스 점검에 성공할 때까지 서비스의 엔드포인트에 포함되지 않음

### 실제 환경에서 레디니스 프로브가 수행하는 기능

- 애플리케이션이 클라이언트 요청을 수신할 수 있는지 여부에 따라 성공/실패 반환
- 서비스에서 파드를 수동으로 추가 or 제거하려면 파드와 서비스의 레이블 셀렉터에 `enabled=true` 레이블을 추가 or 제거한다.
- 레디니스 프로브를 항상 정의하라
  - 레디니스 프로브가 없으면 파드는 시작하는 즉시 서비스 엔드포인트가 된다.
  - 클라이언트의 서비스 요청은 여전히 시작 단계로 준비되지 않은 파드로 전달될 수 있고, 클라이언트는 `Connection refused` 유형의 에러를 마주할 수 있다.
- 파드의 종료코드를 포함하지 마라
  - 쿠버네티스는 파드를 삭제하자마자 모든 서비스에서 파드를 제거하므로 종료 절차에 대한 레디니스 프로브는 불필요하다.

---

## 5.6 헤드리스 서비스로 개별 파드 찾기

> 서비스 연결은 임의의 파드로 전달
> 그런데 클라이언트가 **모든 파드에 연결**해야 하는 경우? **_DNS 조회를 수행하면 각 파드의 IP 주소를 반환_**

DNS 조회 → 하나의 IP 반환

근데 cluster IP를 `None`으로 설정하면 하나의 서비스 IP 대신 **모든 파드 IP를 반환!**

**⇒ 서비스를 헤드리스 상태로 정의하는 것**

```yaml
# kubia-headless.yaml
apiVersion: v1
kind: Service
...
spec:
  clusterIP: None
  ports:
	...
```

### DNS 조회

; 실행 중인 파드 내부에서 수행

1. `tutum/dnsutils` 컨테이너 이미지를 기반으로 새 파드 실행

   1. 별도의 yaml 파일 없이 아래 명령어로 파드 실행

   ```bash
   kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1
   ```

2. DNS 조회

   ```bash
   # service_name: 앞서 배포했던 서비스 yaml에 정의되어 있는 name
   kubectl exec dnsutils nslookup {service_name}
   ```

### ‘모든’ 파드 검색

준비된 파드만 서비스 엔드포인트가 되는데 준비되지 않은 파드까지 검색하려면?

⇒ `publishNotReadyAddresses` annotation을 추가한다.

```yaml

---
kind: Service
metadata:
  annotations:
    publishNotReadyAddresses: "true"
```

---

## 5.7 서비스 문제 해결

KIA 257p. ⇒ 대부분의 서비스 관련 이슈를 해결할 수 있는 방법 제시되어 있음
